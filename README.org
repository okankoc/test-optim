;; This buffer is for notes you don't want to save, and for Lisp evaluation.
;; If you want to create a file, visit that file with C-x C-f,
;; then enter the text in that file's own buffer.

#-*- mode: org -*-
#+STARTUP: showall
#+TODO: WAITING TODO IN-PROGRESS DONE
#+TAGS: { @journal(j) @conference(c) @idea(i) @general(g) }

* Testing unconstrained optimization in C++
"What I cannot create, I do not understand." - Feynman

The goal is to test my own implementations of gradient descent
and Quasi-Newton methods.

Using: make, C++11

** TODO Reading sources
*** TODO Read C++ tips by Herbert Simon
*** TODO Look into Numerical Recipes for efficiency
*** TODO Read proof of Gradient Descent convergence with Line Search
*** TODO Read Newton's Method convergence proof
*** TODO Read QN convergence proof

** DONE Implement linear algebra library
*** DONE Implement vector and matrix types
Including typedefs for the vector matrix types
*** DONE Allocate/deallocate vectors and matrices
Initially using malloc and free
Checking leak with valgrind was very useful.
Moved then to C++ with new/delete
*** DONE Introduce class structure for vectors and matrices
Classes contain raw vector/matrix types
These are std::vectors instead of arrays
Removed destructors after introducing vectors
*** DONE Introduce matrix-vector multiplication
To do that while not violating encapsulation I introduced
inner_product method in Vector class. Let's see if it is
fast enough!
*** DONE Compare speed with Eigen and Armadillo matrices
Comparing 1000x1000 matrix multiplication with 1000x1 vector
Eigen seems to be the fastest on my laptop.
My implementation is of course slower but not too bad!

Asserting condition that it should not be slower than
3 times the time it takes for ARMA/EIGEN!
*** DONE Compare accuracy with Eigen and Armadillo
Comparing the accuracy in a unit test that I created.
Requiring that the entries of the output vector be equal
to the ARMADILLO result. Seeding both ARMA and OWN 
matrices and vectors with C++11 random number generator
and seed.

** IN-PROGRESS Test gradient descent for an arbitrary function
*** DONE Use a learning rate
Added a learning rate, xtol and ftol termination
criteria. I had to also introduce -=, +=, *, and norm
operations on vectors.
*** TODO Add line search
** TODO Test Newton's method with line search
*** TODO Compute Hessian matrix
*** TODO Add matrix inversion
**** TODO Compare speed of matrix inversion Eigen/Arma
**** TODO Test accuracy of matrix inversion
** TODO Add trust region method (Nocedal et al.)
** TODO Add Quasi-Newton method (BFGS)
** TODO Check other libraries
*** TODO Extend vector & matrix computations with BLAS/LAPACK
*** TODO Compare with Eigen unsupported module (MINPACK in C++)
*** TODO Compare with NLOPT routines for speed
