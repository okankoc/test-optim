;; This buffer is for notes you don't want to save, and for Lisp evaluation.
;; If you want to create a file, visit that file with C-x C-f,
;; then enter the text in that file's own buffer.

#-*- mode: org -*-
#+STARTUP: showall
#+TODO: WAITING TODO IN-PROGRESS DONE
#+TAGS: { @journal(j) @conference(c) @idea(i) @general(g) }

* Testing unconstrained optimization in C++
"What I cannot create, I do not understand." - Feynman

The goal is to test my own implementations of gradient descent
and Quasi-Newton methods. In the process I also created
my own Vector and Matrix classes.

Using: cmake, C++11
Testing with: armadillo, Eigen

** IN-PROGRESS Reading sources
*** IN-PROGRESS Read C++ tips by Herbert Simon
*** TODO Look into Numerical Recipes for efficiency
**** TODO Look into decompositions
LU, Cholesky, QR, Eigendecomposition and SVD in particular
**** TODO Look into optimization code
*** DONE Read proof of Gradient Descent convergence with Line Search
The key element in the proof is to bound 
\sum_k cos(\theta_k)^2 \|\grad f_k\|^2

The term appearing in the linear rate of convergence 
depends on the minimum and maximum eigenvalues of the curvature. 

*** DONE Read Newton's Method convergence proof
The Newton's method has a local convergence proof, which
depends on the curvature being lipschitz continuous 
(as a result the function f if C^2)

The global convergence is ONLY achieved if the local
convergence holds and from a far away starting point
the descent directions can be guaranteed with a 
modified matrix factorization approach.
*** DONE Read QN local convergence proof
The QN local convergence relies on the fact that the curvature 
on the QN descent directions p_k are small o(p_k).
*** TODO Read QN global convergence proof
** IN-PROGRESS Implement linear algebra library
*** DONE Implement vector and matrix types
Including typedefs for the vector matrix types
*** DONE Allocate/deallocate vectors and matrices
Initially using malloc and free
Checking leak with valgrind was very useful.
Moved then to C++ with new/delete
*** DONE Introduce class structure for vectors and matrices
Classes contain raw vector/matrix types
These are std::vectors instead of arrays
Removed destructors after introducing vectors
*** DONE Introduce matrix-vector multiplication
To do that while not violating encapsulation I introduced
inner_product method in Vector class. Let's see if it is
fast enough!
*** DONE Compare speed with Eigen and Armadillo matrices
Comparing 1000x1000 matrix multiplication with 1000x1 vector
Eigen seems to be the fastest on my laptop.
My implementation is of course slower but not too bad!

Asserting condition that it should not be slower than
3 times the time it takes for ARMA/EIGEN!
*** DONE Compare accuracy with Eigen and Armadillo
Comparing the accuracy in a unit test that I created.
Requiring that the entries of the output vector be equal
to the ARMADILLO result. Seeding both ARMA and OWN 
matrices and vectors with C++11 random number generator
and seed.

*** TODO Add matrix inversion
**** TODO Compare speed of matrix inversion Eigen/Arma
**** TODO Test accuracy of matrix inversion
** DONE Test gradient descent for an arbitrary function
*** DONE Use a learning rate
Added a learning rate, xtol and ftol termination
criteria. I had to also introduce -=, +=, *, and norm
operations on vectors.
*** DONE Add a common interface to grad descent
Added templated gradient descent function that can work
will ARMADILLO and EIGEN libraries. 

I had to wrap x.norm of Eigen to norm(x) for compatibility. 
In the end there are three specializations which have to be
explicitly spelled out in the source file for the compiler.

**** DONE Add unit tests comparing the results with three libs
Added a unit test for optimizing f(x) = x'*x in ten dimensions.
I had to again introduce three different specializations
for the cost function and its gradient.

*** DONE Add line search
Added templates for gradient descent with line search
and with fixed learning rate.

I had to additionally introduce vdot and vnorm(.) template
functions that are specialized to Vector, arma::vec and
Eigen::VectorXd vectors inside implemenation files 
(i.e. optim_impl_xx.cpp)

*** DONE Test with various test functions
After failing attempts to introduce a concise way
to template multiple test functions (with gradients)
I now wrap them around a structure and introduce
a vector of such structures for gradient descent.

** TODO Test Newton's method
*** TODO Compute Hessian matrix
*** TODO Test with line search
*** TODO Add trust region method (Nocedal et al.)
** TODO Add Quasi-Newton method (BFGS)
** TODO Add extensive test functions and evaluations
I can first check NLOPT to see what functions they are 
testing for unconstrained optim.
*** TODO Check internet for unconstr. optim examples
*** TODO Report termination info
** TODO Check other libraries
*** TODO Extend vector & matrix computations with BLAS/LAPACK
*** TODO Compare with Eigen unsupported module (MINPACK in C++)
*** TODO Compare with NLOPT routines for speed
